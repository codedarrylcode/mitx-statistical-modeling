{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Prediction\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Model Covariance Matrices?**<br>\n",
    "Often, we will be in a situation where we have made some observations of a certain location, but do not have the covariances with another location.\n",
    "\n",
    "One can assume that the covariance between a pair of cities can be determined by its distance (spatial correlation). In this case,\n",
    "\n",
    "$\\displaystyle  \\textsf{Cov}(X_1,X_2) = k(Z_1,Z_2)$\n",
    "\n",
    "where $k(Z_1, Z_2)$ is some covariance function, for example, RBF kernel (squared exponential), which decays exponentially\n",
    "\n",
    "$\\displaystyle  \\textsf{Cov}(X_1,X_2) = k(Z_1, Z_2) = \\exp (- \\frac{\\| Z_1 - Z_2\\| ^2}{2\\ell ^2})$\n",
    "\n",
    "where $\\ell$ is a parameter to be estimated, called the **length-scale**. Covariance functions often go by the name **kernels** ($k$) as well, which are a broader class of functions.\n",
    "\n",
    "$\n",
    "\\displaystyle  \\mathbf{\\Sigma_N} = \\begin{bmatrix}  \\mathbf{k(x_1, x_1)} &  \\mathbf{k(x_1, x_2)}\\\\ \\mathbf{k(x_2, x_1)} &  \\mathbf{k(x_2, x_2)} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "which is also called a kernel ($k$) matrix, $K_N$\n",
    "\n",
    "\n",
    "High-level idea: \n",
    "- Determine a probabilistic model, a multivariate Gaussian with a prior covariance which is decaying with distance\n",
    "- Observe values $y_1, \\dots, y_N$ for some locations, $x_1, \\dots, x_N$\n",
    "- Obtain a posterior distribution for the unobserved variables $Y_*$: $p(Y_* | y_1, \\dots, y_N)$ Gaussian\n",
    "\n",
    "Interpolation with Gaussian processes ([Kriging](https://en.wikipedia.org/wiki/Kriging))\n",
    "- Build a kernel matrix $K_N$ of $N$ observations\n",
    "- For a new point $x_*$: compute a vector $k_*^T = [k(x_*, x_1), \\dots, k(x_*, x_N)]$\n",
    "\n",
    "    $\\mu_{*|1:N} = \\mu_{*} + k_{*}^T K_{N}^{-1} (y_{1:N} - \\mu_{1:N})$\n",
    "    \n",
    "    $\\sigma_{*|1:N}^2 = \\sigma_*^2 - k_{*}^T K_{N}^{-1} k_*$\n",
    "\n",
    "<img alt=\"Kriging\" src=\"assets/kriging.jpg\" width=\"300\">\n",
    "\n",
    "<img alt=\"Kriging with temperature sensors\" src=\"assets/kriging_temperatures.jpg\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Kernel functions**\n",
    "- Helps to predict for any $x_*$, by parameterizing the correlations with a relatively easy and computationally efficient way\n",
    "- Determines the shape of the predicted function\n",
    "- The kernel function applied must yield a covariance matrix, i.e. *symmetric, positive semidefinite*\n",
    "\n",
    "$\\therefore$ A kernel function expresses covariance by a function: $cov(y_i, y_j) = k(x_i, x_j)$\n",
    "\n",
    "This generalizes to all points in our space, which is then called a **Gaussian Process** (GP), a collection of random variables, which represents a joint Gaussian distribution\n",
    "\n",
    "Gaussian Process is fully specified by mean and covariance functions:\n",
    "\n",
    "$m(x) = \\mu_x = \\mathbb {E}[f(x)]$\n",
    "\n",
    "$k(x, x') = cov(f(x), f(x'))$\n",
    "\n",
    "One needs to guarantee that the kernel function's output creates a positive definite matrix or a square of matrices:\n",
    "- If the covariance function is translation invariant, then it is called *stationary*, i.e. if the function depends on $Z_1 - Z_2$\n",
    "- If the covariance function depends only on a norm $\\lvert Z_1 - Z_2 \\rvert$, then it is called isotropic, i.e. depends only on the distance between $Z_1$ and $Z_2$\n",
    "- If the covariance functions depends on $Z_1^T Z_2$, then it is called dot product covariance\n",
    "\n",
    "Examples of kernels and the effects of its parameters:\n",
    "\n",
    "1. Radial Basis Function (RBF) Kernel (*Default*)\n",
    "\n",
    "    $k(x_i, x_j) = \\exp (-\\frac{\\Vert x_i - x_j \\Vert^2}{2 \\ell ^2})$\n",
    "    \n",
    "    If $\\ell$ is large, then distance divided by $\\ell$ goes to zero and there will still be large covariance values between two points that are far apart. For small $\\ell$, each point is interpolated with only its closest neighbours and therefore *varies quickly*.\n",
    "    \n",
    "    <img alt=\"RBF Kernel and $\\ell$ parameter\" src=\"assets/rbf_kernel.jpg\" width=\"300\">\n",
    "    \n",
    "    <img alt=\"RBF Kernel and $\\ell$ parameter 2\" src=\"assets/rbf_kernel_2.jpg\" width=\"600\">\n",
    "    \n",
    "    \n",
    "2. Gamma-exponential Kernel\n",
    "\n",
    "    Helps to alter the sharpness/shape of the covariance function against the distance between two points\n",
    "    \n",
    "    $k(x_i, x_j) = \\exp (-(\\frac{\\Vert x_i - x_j \\Vert^2}{2 \\ell ^2})^\\gamma)$\n",
    "    \n",
    "    <img alt=\"Gamma-exponential Kernel\" src=\"assets/gamma_exponential.jpg\" width=\"600\">\n",
    "    \n",
    "\n",
    "3. Polynomial Kernel\n",
    "\n",
    "    Linear kernel: $k(x, x') = \\langle x, x' \\rangle$\n",
    "    \n",
    "    Quadratic kernel: $k(x, x') = (\\langle x, x' \\rangle + 1)^2$\n",
    "    \n",
    "    <img alt=\"Polynomial Kernel\" src=\"assets/polynomial_kernel.jpg\" width=\"600\">\n",
    "    \n",
    "    \n",
    "4. Periodic Kernel\n",
    "\n",
    "    $k(x, x') = \\exp (-\\frac{2\\sin^2 (\\pi (x - x') / p)}{2 \\ell ^2})$\n",
    "    \n",
    "    where $p$ is a period paramter, which states that the value of a point will default back to the previous period, for e.g. seasonal temperature\n",
    "    \n",
    "    This has advantages over the RBF Kernel when it comes to extrapolation for periodic data. RBF does well interpolating when data points are close but has too much uncertainty when moving away from data points (extrapolation)\n",
    "    \n",
    "    <img alt=\"Periodic Kernel\" src=\"assets/periodic_kernel.jpg\" width=\"600\">\n",
    "    \n",
    "5. Other possible kernel functions\n",
    "\n",
    "    *Assume here that $x$ is a difference between points, e.g $x = Z_1 - Z_2$, and $r$ is a distance, e.g $r = \\Vert Z_1 - Z_2 \\Vert$*\n",
    "    \n",
    "    - Constant, $\\sigma_0^2$\n",
    "    - Linear, $\\sum_{d=1}^{D} \\sigma_d^2 x_d x_d'$\n",
    "    - Squared exponential, $\\exp (- \\frac{r^2}{2 \\ell ^2})$\n",
    "    - Mat√©rn, $\\frac{1}{2^{\\nu - 1} \\Gamma(\\nu)} (\\frac{\\sqrt{2 \\nu}}{\\ell} r)^{\\nu} K_{\\nu} (\\frac{\\sqrt{2 \\nu}}{\\ell} r)$\n",
    "    - Exponential, $\\exp (- \\frac{r}{\\ell})$\n",
    "    - Rational quadratic, $(1 + \\frac{r^2}{2 \\alpha \\ell^2})^{-\\alpha}$\n",
    "    - Neural network, $\\sin ^{-1}\\left( \\frac{2\\mathrm{{\\boldsymbol x}}^{\\intercal }{\\boldsymbol \\Sigma }\\mathrm{{\\boldsymbol x}}}{\\sqrt{(1 + 2\\mathrm{{\\boldsymbol x}}^{\\intercal }{\\boldsymbol \\Sigma }\\mathrm{{\\boldsymbol x}})(1 + 2{\\mathrm{{\\boldsymbol x}}^\\prime }^{\\intercal }{\\boldsymbol \\Sigma }\\mathrm{{\\boldsymbol x}}')}} \\right)$\n",
    "\n",
    "\n",
    "How to build more covariance functions? Some processes are a mix of functions and therefore we may have to estimate the covariance structure with a mixed function.\n",
    "\n",
    "Some ways to build kernel functions:\n",
    "\n",
    "- A sum of kernel functions is a kernel function, $k(x, x') = k_1 (x, x') + k_2 (x, x')$, e.g. $k_{linear} + k_{periodic}$\n",
    "- A product of kernel functions is a kernel function, $k(x, x') = k_1 (x, x') \\cdot k_2 (x, x')$\n",
    "    - $k_{linear} \\cdot k_{periodic}$, where the periodicity is scaled higher and lower by the distance\n",
    "    - $k_{RBF} \\cdot k_{periodic}$\n",
    "    - $k((x_1, x_2, t), (x_1', x_2', t')) = k_{space} ((x_1, x_2), (x_1', x_2')) \\cdot k_{time} (t, t')$\n",
    "\n",
    "****\n",
    "\n",
    "**Effect of Measurement Noise on Gaussian Processes**\n",
    "\n",
    "1. Stationarity\n",
    "\n",
    "    Given the ground truth, that may have large varaince in low $x$ values and small variance in high $x$ values\n",
    "    <img alt=\"Non-Stationarity\" src=\"assets/nonstationary_kernels.jpg\" width=\"300\">\n",
    "\n",
    "    Use a $\\log$ transformation, for e.g., $k(x, x') = \\exp (- \\Vert \\log(0.1 + x) - \\log(0.1 + x') \\Vert^2)$\n",
    "\n",
    "    <img alt=\"Log-Transformation\" src=\"assets/log_transformation_kernel.jpg\" width=\"300\">\n",
    "    \n",
    "2. Noise in observations, $\\tau$\n",
    "\n",
    "    Given observations, $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim N(0, \\tau^2)$ are iid\n",
    "    \n",
    "    How does noise affect variance / covariance?\n",
    "    \n",
    "    $cov(y_i, y_j) = cov(y_i' + \\epsilon_i, y_j' + \\epsilon_j) = cov(y_i' + y_j') + cov(\\epsilon_i, \\epsilon_j) = k(x_i, x_j) + 0$, if $i \\neq j$ ($\\tau^2$, if $i = j$), \n",
    "    \n",
    "    where $y_i', y_j'$ are ground-truth of $y$\n",
    "    \n",
    "    $\\therefore$ A new covariance matrix of observed $y_i, \\dots, y_N$: $K_N + \\tau^2 \\mathbb{1}$, where we add $\\tau^2$ on the diagonal of the covariance matrix. Follow the rest as per regular sequence.\n",
    "    \n",
    "    \n",
    "****\n",
    "\n",
    "**General comments on Gaussian Processes**\n",
    "\n",
    "- Flexible, nonlinear regression method\n",
    "- Kernel determins what kind of function we fit\n",
    "- Applicable far beyond spatial models, many settings of nonlinear regression (including time series)\n",
    "- Reducing uncertainty, e.g. can help guide sensor placement to maximize information by reducing uncertainty, *Bayesian Optimization*\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
