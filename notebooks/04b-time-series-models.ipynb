{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series: Statistical Models & Fitting\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap**<br>\n",
    "\n",
    "Weak stationarity is defined as:\n",
    "\n",
    "1. Mean, variance independent of $t$: $\\mu_X (t) = \\mu$, $var_X (t) = var_X$\n",
    "\n",
    "2. Autocovariance is just a function of the time distance, *e.g. autocovariance between Jan and Feb should be the same as Oct and Nov*: $\\gamma_X (s, t) = \\gamma(\\lvert s - t \\rvert)$\n",
    "\n",
    "To check for stationarity, under appropriate technical conditions, the distribution of the estimator is:\n",
    "\n",
    "$\\hat\\gamma_W (h) \\sim N(0, \\frac{\\sigma_W^2}{n})$\n",
    "\n",
    "which means that the autocovariance, for $h > 0$ is expected to be close to zero if the series is stationary. If the autocovariance function does not decay to zero at all, or decays to zero very slowly, it is an indication of nonstationarity\n",
    "\n",
    "<img alt=\"Stationarity\" src=\"assets/stationarity.png\" width=\"300\">\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Model 1: Autoregressive, $AR(p)$**\n",
    "\n",
    "$X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\dots \\phi_p X_{t-p} + W_t$\n",
    "\n",
    "AR(1) means that the model will use $t-1$ time steps to predict $X_t$ by estimating $\\phi$ that minimizes the error.\n",
    "\n",
    "The model is usually not very useful for long term prediction as it converges to a constant value (the unconditional mean of the time series). To get longer term prediction, increase the time step between different measurements. For example, with daily time series data, it may be hard to predict 30 days ahead; but if we first average the daily data into weekly data then we might be able to predict the value for next month (with 4 time steps ahead). Another way is to model the trend and seasonality seperately - this will assume that the trend and seasonality will persist in the long-term.\n",
    "\n",
    "****\n",
    "\n",
    "**Model 2: Random Walk**\n",
    "\n",
    "$X_t = X_{t-1} + W_t + \\delta$ where $\\delta$ is a deterministic linear increase at each time step\n",
    "\n",
    "Essentially, this is a sum of white noise random variables:\n",
    "\n",
    "$X_t = X_{t-1} + W_t = X_{t-2} + W_{t-1} + W_t = X_0 + W_1 + \\dots + W_t$\n",
    "\n",
    "With drift (linear increase):\n",
    "\n",
    "$X_t = t \\cdot \\delta + X_0 + \\sum_{s=0}^{t} W_s$\n",
    "\n",
    "<img alt=\"Random Walk with Drift\" src=\"assets/random_walk_drifted.png\" width=\"300\">\n",
    "\n",
    "Properties:\n",
    "\n",
    "- $\\mathbb {E}[X_t] = t \\cdot \\delta + X_0$\n",
    "- without drift: $var(X_t) = t \\cdot \\sigma_w^2$\n",
    "- Not stationary, because its expectation and variance grows with $t$\n",
    "- but $\\nabla X_t = X_t - X_{t-1}$ is stationary\n",
    "- autocovariance: $\\gamma_X (s,t) = cov(X_s, X_t) = var(X_0) + \\min(s,t) \\cdot \\sigma_W^2$\n",
    "\n",
    "****\n",
    "\n",
    "**Model 3: Moving Average, $MA(q)$**\n",
    "\n",
    "Given $q$, uses the previous $q$ white noises to predict the next position:\n",
    "\n",
    "$X_t = W_t + \\theta_1 W_{t-1} + \\dots + \\theta_q W_{t+q}$\n",
    "\n",
    "Properties:\n",
    "\n",
    "- $\\mathbb {E}[X_t] = 0$\n",
    "- autocovariance $\\gamma (s, t)$ depends only on $\\lvert s - t \\rvert$ and is therefore stationary\n",
    "\n",
    "    $\\gamma_X (h) = Cov(\\sum_{j=0}^{q} \\theta_j W_{t-j}, \\sum_{k=0}^{q} \\theta_k W_{t+h-k}) = \\sum_{j = 0}^{q-h} \\theta_j \\theta_{j+h} \\sigma_W^2$\n",
    "    \n",
    "    \n",
    "- ACF reflects order: $\\gamma(s,t) = 0$ if $\\lvert s - t \\rvert > q$\n",
    "- ACF distinguishes MA and AR models where ACF goes to 0 when time distance is more than order for MA models but ACF decays exponentially as time distance increases for AR models \n",
    "\n",
    "****\n",
    "\n",
    "**Model 4: ARMA(p,q)**\n",
    "\n",
    "$X_t = \\phi_1 X_{t-1} + \\dots + \\phi_p X_{t-p} + W_t + \\theta_1 W_{t-1} + \\dots + \\theta_q W_{t-q}$\n",
    "\n",
    "**Model 5: ARIMA(p,d,q)**\n",
    "\n",
    "Additional $d$ term for differencing order in addition to autoregressive and moving average terms.\n",
    "\n",
    "****\n",
    "\n",
    "**Regression x Time series**\n",
    "\n",
    "- General model: $X_t = \\beta^T \\cdot z_t + W_t$\n",
    "- linear trend: $X_t = \\beta_1 + \\beta_2 t = W_t$\n",
    "- AR(2) model: $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + W_t$\n",
    "- external regressors: $X_t = \\beta_1 X_{t-1} + \\beta_2 Y_t + W_t$, where $Y_t$ is an external variable\n",
    "\n",
    "Using least squares estimate: $\\min_{\\beta} \\sum_{t} (x_t - \\beta^T z_t)^2$, but errors may be correlated over time as observations are not orthogonal.\n",
    "\n",
    "How to decide which model to use? Use ACF as diagnostic tool.\n",
    "\n",
    "Example: $X_t = T_t + Y_t$, sum of\n",
    "- linear trend: $T_t = 50 + 3t$\n",
    "- AR(1) model: $Y_t = 0.8Y_{t-1} + W_t$, $\\sigma_W = 20$\n",
    "\n",
    "<img alt=\"Sample Time Series\" src=\"assets/sample_time_series.jpg\" width=\"300\">\n",
    "\n",
    "Look at largest cross-covariance terms to determine which variables to select\n",
    "\n",
    "<img alt=\"Cross Covariance\" src=\"assets/cross_covariance.jpg\" width=\"300\">\n",
    "\n",
    "Generally, follow these steps when fitting a time series:\n",
    "\n",
    "1. Transform time series to make it stationary:\n",
    "    - log-transform\n",
    "    - remove trends / seasonality\n",
    "    - differencing\n",
    "2. Check if time series looks like only white noise using ACF\n",
    "3. Otherwise, fit MA models if ACF decays exponentially or AR models if ACF decays linearly\n",
    "4. Estimate coefficients and compute residuals to test for white noise\n",
    "\n",
    "****\n",
    "\n",
    "**Parameter estimation for stationary AR(p), using Yule-Walker**\n",
    "\n",
    "Estimate parameters $\\hat\\phi$, $\\hat\\sigma_W^2$ using Yule-Walker equations: method of moments\n",
    "\n",
    "1. Estimate autocovariances $\\gamma(h)$ for $h = 0, 1, 2, \\dots$ from averages\n",
    "2. Solve system of linear equations: for $h= 1, \\dots, p$ using estimates of $\\gamma(h)$\n",
    "    \n",
    "    $\\gamma(h) = \\phi_1 \\gamma(h-1) + \\phi_2 \\gamma(h-2) + \\dots + \\phi_p \\gamma(h-p)$\n",
    "\n",
    "    $\\sigma_W^2 = \\gamma(0) - \\phi_1 \\gamma(1) - \\phi_2 \\gamma(2) - \\dots - \\phi_p \\gamma(p)$\n",
    "    \n",
    "- Yule-Walker equations in matrix form ($\\Gamma_p$) is a $p \\times p$ covariance matrix with $(i, j)$th entry is $\\gamma (i - j)$\n",
    "- Using column vectors $\\gamma_p = [\\gamma(1), \\dots, \\gamma(p)]^T$\n",
    "- Solve for $\\phi$ with this equation:\n",
    "\n",
    "    $\\gamma_p = \\Gamma_p \\phi$\n",
    "    \n",
    "    $\\phi = \\Gamma_p^{-1} \\gamma_p$\n",
    "    \n",
    "    \n",
    "- Solve for $\\sigma_W^2$ with this equation:\n",
    "\n",
    "    $\\sigma_W^2 = \\gamma(0) - \\phi_1 \\gamma(1) - \\phi_2 \\gamma(2) - \\dots - \\phi_p \\gamma(p) = \\gamma(0) - \\hat\\phi^T \\gamma_p = \\gamma(0) - \\gamma_p^T \\Gamma_p^{-1} \\gamma_p$\n",
    "\n",
    "****\n",
    "\n",
    "**Forecasting with AR(p) model**\n",
    "\n",
    "- Estimate $M$ steps into the future based on $N$  observations\n",
    "- Estimate coefficients $\\hat\\phi_1, \\dots, \\hat\\phi_p$ and plug in:\n",
    "\n",
    "    $\\hat X_{n+1} = \\hat\\phi_1 X_n + \\dots +\\hat\\phi_p X_{n-p+1}$\n",
    "    \n",
    "    $X_{n+2} = \\hat\\phi_1 \\hat X_{n+1} + \\hat\\phi_2 X_n + \\dots +\\hat\\phi_p X_{n-p+2}$\n",
    "\n",
    "    Note: When plugging in $\\hat X_{n+1}$ then we are just plugging in $\\hat\\phi_1 X_n + \\dots +\\hat\\phi_p X_{n-p+1}$\n",
    "    \n",
    "    $\\therefore$ it is always a linear combination of last $p$ observations, $X_n, \\dots, X_{n-p+1}$\n",
    "    \n",
    "    **Caution**: Only works for short horizons $m$. For long horizons, it actually converges to the mean\n",
    "    \n",
    "    <img alt=\"Convergence to the mean\" src=\"assets/converges_to_mean.jpg\" width=\"300\">\n",
    "\n",
    "****\n",
    "\n",
    "**Using Partial Autocorrelation Function, PACF, to determine order, $p$ for AR(p)**\n",
    "\n",
    "For an AR(1) model, the ACF may decay slowly to zero and does not deterministically tell us what is the order that should be applied - unlike the MA(q) model.\n",
    "\n",
    "Example: \n",
    "\n",
    "$AR(1): X_t = \\phi X_{t-1} + W_t = \\phi^2 X_{t-2} + \\phi W_{t-1} + W_t$\n",
    "\n",
    "$Corr(X_t, X_{t-2}) = Corr(\\phi^2 X_{t-2} + \\phi W_{t-1} + W_t, X_{t-2}) = \\phi^2 \\gamma(0)$\n",
    "\n",
    "\n",
    "Partial correlation of $X$, $Y$ given $Z$:\n",
    "- Regress X on Z; Y on Z\n",
    "- $\\rho_{XY \\vert Z} = corr(X - \\hat X, Y - \\hat Y)$ -- Therefore this captures the relationship of $X$, $Y$ beyond $Z$\n",
    "- Formally, the partial autocorrelation of time series $X_t$ at lag $h$ is:\n",
    "\n",
    "    $\\alpha_X (h) := Corr(X_h - \\hat X_h^{lin_{h-1}}, X_0 - \\hat X_0^{lin_{h-1}})$\n",
    "    \n",
    "    where $\\hat X_h^{lin_{h-1}}$ is the linear regression projection of $X_h$ on $X_1, \\dots, X_{h-1}$ and $\\hat X_0^{lin_{h-1}}$ is the linear regression projection of $X_0$ on $X_1, \\dots, X_{h-1}$.\n",
    "    \n",
    "    $\\therefore$ $\\alpha_X (h)$ is the correlation between $X_h$ and $X_0$ after removing the linear predictions based on the intermediate terms of the series $X_1, \\dots, X_{h-1}$\n",
    "    \n",
    "    A convenient way to compute the partial autocorrelation $\\alpha_X (h)$ is to use the Frisch-Waugh-Lovell theorem. FWL theorem says that $\\alpha_X (h)$ is the regression coefficient on regressor $X_{t-h}$ in the regression of $X_t$ along with all intermediate terms, i.e.\n",
    "    \n",
    "    $X_t = \\phi_1 X_{t-1} + \\dots + \\phi_h X_{t-h} + \\tilde X_t$\n",
    "    \n",
    "    then $\\alpha_X (h) = \\phi_h$ where the regression coefficients $\\phi_1, \\dots, \\phi_h$ can be estimated by the method of moments with Yule-Walker equations.\n",
    "    \n",
    "****\n",
    "\n",
    "**Information Criterion and Cross-Validation, to evaluate model fit for time series**    \n",
    "\n",
    "1. Akaike Information Criterion\n",
    "\n",
    "    Main idea is to tradeoff between model fit and complexity.\n",
    "\n",
    "    $\\displaystyle  AIC= 2k - 2ln(L)$, where $\\displaystyle ln(L)$ refers to the log-likelihood of the model fit and $k$ refers to the number of terms used in the model fit (works as a penalty)\n",
    "    \n",
    "    Models with smaller AIC values are preferred and are associated with smaller number of model parameters whilst maximizing the likelihood value. Comparing with using BIC, typically complex models are more preferable with AIC than with BIC since BIC usually has a larger penalty on model complexity\n",
    "    \n",
    "\n",
    "2. Cross Validation\n",
    "\n",
    "    Randomly partition into $k$ folds and train on $k-1$ folds, test on remaining fold. Repeat until all folds have been used as a test-set and find the expectation of the validation error as a measure of the model fit. Generally more preferable as it has fewer assumptions compared to the model selections relied on Information Criterion but requires larger sample sizes and computational speed.\n",
    "    \n",
    "    Tricky for time-series data as cross validation assumes independent observations that can be partitioned into folds, but time-series data typically aren't independent across time. This technique is therefore typically avoided in time-series.\n",
    "    \n",
    "    Workaround: Sample a window and train/test repeatedly\n",
    "    \n",
    "    <img alt=\"Cross Validation for Time Series\" src=\"assets/cross_validation_time_series.jpg\" width=\"300\">\n",
    "    \n",
    "****\n",
    "\n",
    "**Linear Process: The relationship between MA vs AR**\n",
    "\n",
    "Since AR model is $X_t = \\Phi \\cdot X_{t-h} + W_t$, where $X_{t-h}$ contains white noise in each term, and MA model is $X_t = \\Theta \\cdot W_{t-h} + W_t$, a linear combination of white noises, both models are linear, causal (function of the past) processes.\n",
    "\n",
    "Example: AR model, $X_t = W_t + \\phi_1 X_{t-1} = \\sum_{j=0}^{\\infty} \\phi_1^j W_{t-j}$\n",
    "\n",
    "MA(1) essentially is an infinite AR process (unders similar convergence conditions) in the form:\n",
    "\n",
    "$W_t = X_t - \\phi_1 X_{t-1} - \\dots$\n",
    "\n",
    "****\n",
    "\n",
    "*Extra references: [Subseasonal Weather Forecasting using non-linear models](https://dl.acm.org/doi/10.1145/3292500.3330674)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.394829814011908 12.368978884071453\n",
      "64.47238260383328 110.52408446371419\n"
     ]
    }
   ],
   "source": [
    "# Compute AIC/BIC by hand\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "k_a = 10\n",
    "log_likelihood_a = 10\n",
    "k_b = 20\n",
    "log_likelihood_b = 10**6\n",
    "\n",
    "aic_a = 2*k_a - 2*np.log(log_likelihood_a)\n",
    "aic_b = 2*k_b - 2*np.log(log_likelihood_b)\n",
    "bic_a = k_a * np.log(n) - 2*np.log(log_likelihood_a)\n",
    "bic_b = k_b * np.log(n) - 2*np.log(log_likelihood_b)\n",
    "\n",
    "print(aic_a, aic_b)\n",
    "print(bic_a, bic_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
