{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of High-Dimensional Data\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Algebra Review**<br>\n",
    "A matrix $M$ is an orthogonal projection onto a subspace $S$ if\n",
    "1. $M$ is symmetric\n",
    "1. $M^2 = M$, and\n",
    "1. $S =$ {$y: Mx = y$ for some $x \\in R^n$}\n",
    "\n",
    "In general, the projection of a vector $x \\in R^d$ onto a unit vector $u$ is defined to be\n",
    "$proj_u x := (u \\cdot x) \\cdot u$\n",
    "\n",
    "To form the unit vector, take the original vector $v$ and divide by its norm $\\lVert v \\rVert$\n",
    "\n",
    "$u = \\frac{v}{\\lVert v \\rVert}$\n",
    "\n",
    "The empirical variance of a dataset in a given direction is calculated by projecting each obervation by the unit vector of the given direction and then calculating its variance. This turns out to be the same as calculating its empirical covariance matrix first before projecting it with the unit vector.\n",
    "\n",
    "$Var(u \\cdot X_1, \\dots, u \\cdot X_n) = u^T S u$\n",
    "\n",
    "where $S = Cov(X) = \\frac{1}{n-1} \\sum_{i = 1}^{N} x_i x_i^T$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Orthogonal matrices**\n",
    "\n",
    "A matrix $P \\in R^{d x d}$ is orthogonal if $PP^T = P^TP = I_d$. If $A = PDP^T$, where $D$ is a positive definite diagonal matrix and $P$ is an orthogonal matrix with column vectors, $v_1, \\dots, v_n$, then $A \\cdot v_1 = \\lambda_1 \\cdot v_1$ and $A$ is a symmetric, positive definite matrix\n",
    "\n",
    "For all symmetric matrices, $A$, with real entries, $A = PDP^T$\n",
    "\n",
    "where\n",
    "- $P \\in R^{d x d}$ is an orthogonal matrix with eigenvectors as column vectors, and\n",
    "- $D \\in R^{d x d}$ is a diagonal matrix with eigenvalues on its diagonals\n",
    "\n",
    "*Remark 1*: \n",
    "\n",
    "Positive semidefinite matrices and orthogonal matrices have mathematically different properties. In particular, most positive semidefinite matrices are not orthogonal, and most orthogonal matrices are not positive semidefinite. For example, a covariance matrix is a symmetric and positive semidefinite matrix but is not an orthogonal matrix.\n",
    "\n",
    "*Remark 2*: \n",
    "\n",
    "In PCA, the strategy is to diagonalize the matrix (empirical covariance) and select the eigenvectors whose eigenvalues are the largest. This gives us the primary axes that captures the maximum variance about the data.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Principal Component Analysis (PCA)**<br>\n",
    "Reduce to a few dimensions which contains the largest spread/information about the data.\n",
    "\n",
    "$X^T X = \\Sigma = V \\Lambda V^T$\n",
    "\n",
    "Definitions:\n",
    "1. Maximize projection variance (direction with largest variance)\n",
    "1. Minimize projection residuals (smallest orthogonal distance to all points)\n",
    "1. Spectral decomposition\n",
    "    1. Given covariance matrix, $S$, a symmetric and positive semidefinite matrix, \n",
    "    1. Decompose it into $S = V \\Lambda V^T$\n",
    "        - $\\Lambda$ is a diagonal matrix with eigenvalues (variances along PCs) of $S$\n",
    "        - $V$ is orthogonal matrix with eigenvectors of $S$\n",
    "        \n",
    "    1. Find the directions (eigenvectors) with the largest eigenvalues.\n",
    "\n",
    "Find a projection vector, $u$, such that it maximizes the empirical variance spanned by $u$, i.e. the principal axes that captures the maximum variance/information about the data.\n",
    "\n",
    "$\\displaystyle\\arg \\max_{\\substack{u:\\lVert u \\rVert^2_2 = 1}} u^T S u$\n",
    "\n",
    "Practical tips:\n",
    "- Use correlation matrix instead of covariance to ensure robustness against scale\n",
    "- Keep principal components where eigenvalues (of the correlation matrix) are larger than 1 which indicates that the particular principal component has above average variance explained\n",
    "- Total variance explained by the principal components is the sum of the eigenvalues of the covariance matrix\n",
    "    - $\\therefore$ %Variance explained by a principal component is given by $\\frac{\\lambda_i}{\\sum_{j = 1}^{p} \\lambda_j}$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Mutidimensional Scaling (MDS)**<br>\n",
    "MDS is a technique for reducing data dimensions while attempting to preserve the original **euclidean** distances between high-dimensional data points\n",
    "\n",
    "Given a distance matrix $D \\in R^{n x n}$ between $n$ data points, map points $y_1, \\dots, y_n \\in R^q$ where $q$ is small (2 or 3)\n",
    "\n",
    "- Classical MDS: Minimize $\\sum_{i=1}^{n} \\sum_{j=1}^{n} (D_{ij} - \\lVert y_i - y_j \\rVert_2)^2$, where $D$ is a Euclidean distance matrix between dimensions\n",
    "- Weighted MDS: Add a weighting with $w_{ij} \\geq 0$\n",
    "- Non-metric MDS: Minimize $\\sum_{i=1}^{n} \\sum_{j=1}^{n} (\\theta(D_{ij}) - \\lVert y_i - y_j \\rVert_2)^2$ where D is a dissimilarity matrix (rankings) and $\\theta$ is an increasing function that does not alter the rankings\n",
    "    - Able to find low-dimensional embedding that respects rankings\n",
    "    - Very time-consuming as it needs to be optimized over increasing function, $\\theta$\n",
    "    \n",
    "General approach of Classical MDS:<br>\n",
    "First convert a distance matrix $D$, with $D_{ij} = \\lVert x_i - x_j \\rVert_2$ into a positive semidefinite matrix $XX^T$, namely the [Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix)\n",
    "\n",
    "$B = XX^T = -\\frac{1}{2} (I - \\frac{1}{n} 11^T) \\cdot D^2 \\cdot (I - \\frac{1}{n} 11^T)$, where $1$ is a vector of ones and $D$ is a matrix with element $D_{ij}^2 = (d_{ij})^2 = (XX^T)_{ii} + (XX^T)_{jj} - 2(XX^T)_{ij}$\n",
    "\n",
    "Objective: Find a low-dimensional ($q$) embedding, $Y$, that best represents the data\n",
    "\n",
    "$\\displaystyle\\arg \\min_{\\substack{Y}} trace(XX^T - YY^T)^2$ - the Frobenius norm\n",
    "\n",
    "Eigenvalue decomposition of the Gram matrix: $XX^T = V \\Lambda V^T$, where columns of $V$ are eigenvectors of $XX^T$, $\\Lambda$ is a diagonal containing non-negative eigenvalues of $XX^T$, as a Gram matrix is a positive semidefinite matrix\n",
    "\n",
    "Keep the best rank $q$ approximation of $XX^T$ is given by choosing $q$ largest eigenvalues and corresponding eigenvectors, i.e. $YY^T = V_1 \\Lambda_1 V_1^T$, or equivalently, $Y = V_1 \\Lambda^{\\frac{1}{2}}_1$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Stochastic Neighbour Embedding (SNE)**<br>\n",
    "SNE (non-convex) is a non-linear technique to â€œcluster\" data points by trying to keep similar data points close to each other.\n",
    "\n",
    "- Place a standard Gaussian on each observation in high-dimensional space\n",
    "- Find embedding such that distribution is approximated well by low-dimensional embedding (minimize KL divergence)\n",
    "\n",
    "General Approach:\n",
    "\n",
    "1. Given dissimilarity matrix $D$, for each observation, $i$, compute probability of picking $j$ as neighbour:\n",
    "\n",
    "    $p_{ij} = \\frac{exp(-D_{ij}^2)}{\\sum_{k \\neq l} exp(-D_{kl}^2)}$\n",
    "    \n",
    "    where $D_{ij}^2 = \\lVert x^{(i)} - x^{(j)} \\rVert^2$, $i \\neq j$ and the denominator sums over all distinct pairs of data points.\n",
    "    \n",
    "    The set of all $p_{ij}$ defines the PMF on all pairs of points in p-dimensional space. Pairs that are close together (i.e. close to 0 in a Gaussian) are given much more weight.\n",
    "    \n",
    "\n",
    "2. In low-dimensional space, compute probability of $y_j$ as neighbour:\n",
    "\n",
    "    $q_{ij} = \\frac{exp(- \\lVert y_i - y_j \\rVert_2^2)}{\\sum_{k \\neq l} exp(- \\lVert y_k - y_l \\rVert_2^2)}$\n",
    "    \n",
    "\n",
    "3. Find points, $y^{(i)}$ by minimizing the KL-divergence, using gradient descent:\n",
    "\n",
    "    $KL(P \\lVert Q) = \\sum_{i \\neq j} p_{ij} log (\\frac{p_{ij}}{q_{ij}})$\n",
    "    \n",
    "\n",
    "Mathematical intuition: \n",
    "\n",
    "Small distance between $i$, $j$ then $D_{ij}$ is small, $p_{ij}$ is large. If distance in low-dimensional embedding is large, then $q_{ij}$ is small then KL divergence between $P$ and $Q$ becomes large.\n",
    "\n",
    "\n",
    "Problem: Projecting into a low-dimensional space causes points to get 'crowded'\n",
    "\n",
    "Solution: **t-SNE**\n",
    "\n",
    "Instead of a Gaussian, use a t-distribution with 1 degree of freedom. This moderates the distance and reduces crowding. The axes of a tSNE does not have any meaning.\n",
    "\n",
    "<img alt=\"Mutidimensional Scaling vs t-SNE\" src=\"assets/mds_vs_tsne.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear Algebra & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T10:34:40.044758Z",
     "start_time": "2021-09-20T10:34:40.034533Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical Mean:  [5.5  5.   3.25]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the empirical mean and covariance\n",
    "import numpy as np\n",
    "\n",
    "X1 = np.array([8, 4, 7])\n",
    "X2 = np.array([2, 8, 1])\n",
    "X3 = np.array([3, 1, 1])\n",
    "X4 = np.array([9, 7, 4])\n",
    "X  = np.vstack([X1, X2, X3, X4])\n",
    "\n",
    "X_bar = np.mean(X, axis = 0)\n",
    "print('Empirical Mean: ', X_bar)\n",
    "X_sigma = np.cov(X.T, bias = True) # Columns are observations, rows are dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T10:24:42.108599Z",
     "start_time": "2021-09-20T10:24:42.098742Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.33333333 -0.66666667 -1.66666667]\n",
      "[ 2.33333333 -0.66666667 -1.66666667]\n"
     ]
    }
   ],
   "source": [
    "# Orthogonal projection\n",
    "d = 3\n",
    "H = np.eye(d) - 1/d * np.outer(np.ones(d), np.ones(d))\n",
    "X = np.array([2, -1, -2])\n",
    "\n",
    "print(H.dot(X))\n",
    "print(H.dot(H).dot(X))\n",
    "\n",
    "## Therefore H is an orthogonal projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T11:08:39.114701Z",
     "start_time": "2021-09-20T11:08:39.103132Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2 -0.4]\n",
      "4.8\n",
      "4.799999999999999\n"
     ]
    }
   ],
   "source": [
    "# Vector projections\n",
    "X1 = np.array([1,2])\n",
    "X2 = np.array([3,4])\n",
    "X3 = np.array([-1,0])\n",
    "X  = np.stack([X1, X2, X3])\n",
    "u  = np.array([1,2]) * 1/np.sqrt(5)\n",
    "\n",
    "# (u.x).u\n",
    "print(np.outer(u, X3).dot(u))\n",
    "\n",
    "# Using u^T S u to compute projected empirical variance\n",
    "n = X.shape[0]\n",
    "S = (1/n * X).T.dot(np.eye(n) - 1/d * np.outer(np.ones(n), np.ones(n))).dot(X)\n",
    "print(u.dot(S).dot(u))\n",
    "\n",
    "# Using product of each vector projections to compute empirical variance \n",
    "print(np.std([np.dot(u, X1), np.dot(u, X2), np.dot(u, X3)])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T10:43:33.518050Z",
     "start_time": "2021-09-20T10:43:33.511524Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 2.]]\n",
      "[0. 2.]\n",
      "------------------------------\n",
      "Direction with maximum spread (PC1):\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Find eigenvalues and PC1 of a covariance matrix\n",
    "data = np.array([[0,1], \n",
    "                 [0,-1]])\n",
    "cov  = np.cov(data.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "max_idx = np.argmax(eigenvalues)\n",
    "\n",
    "print(cov)\n",
    "print(eigenvalues)\n",
    "print('-'*30)\n",
    "print('Direction with maximum spread (PC1):')\n",
    "print(eigenvectors[:, max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T10:55:58.956616Z",
     "start_time": "2021-09-20T10:55:58.948868Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.  1. ]\n",
      " [1.  0.5]]\n",
      "[2.5 0. ]\n",
      "------------------------------\n",
      "Direction with maximum spread (PC1):\n",
      "[0.89442719 0.4472136 ]\n"
     ]
    }
   ],
   "source": [
    "# Practice\n",
    "data = np.array([[1,.5], \n",
    "                 [-1,-.5]])\n",
    "cov  = np.cov(data.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "max_idx = np.argmax(eigenvalues)\n",
    "\n",
    "print(cov)\n",
    "print(eigenvalues)\n",
    "print('-'*30)\n",
    "print('Direction with maximum spread (PC1):')\n",
    "print(eigenvectors[:, max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T11:05:29.331672Z",
     "start_time": "2021-09-20T11:05:29.325438Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1180339887498947"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projection\n",
    "PC1 = eigenvectors[:, max_idx]\n",
    "\n",
    "np.linalg.norm(\n",
    "     np\n",
    "     .outer(PC1, data[1])\n",
    "     .dot(PC1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T11:16:41.213236Z",
     "start_time": "2021-09-20T11:16:41.205917Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 3.]]\n",
      "[1. 3.]\n",
      "------------------------------\n",
      "Direction with maximum spread (PC1):\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Practice\n",
    "data = np.array([[0,2], \n",
    "                 [1,-1],\n",
    "                 [-1,-1]\n",
    "                ])\n",
    "cov  = np.cov(data.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "max_idx = np.argmax(eigenvalues)\n",
    "\n",
    "print(cov)\n",
    "print(eigenvalues)\n",
    "print('-'*30)\n",
    "print('Direction with maximum spread (PC1):')\n",
    "print(eigenvectors[:, max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T11:27:11.823620Z",
     "start_time": "2021-09-20T11:27:11.819052Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.]\n",
      " [-0. -1.]\n",
      " [-0. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# Projecting dataset into PC1\n",
    "PC1 = eigenvectors[:, max_idx]\n",
    "projection = np.outer(PC1.dot(data.T), PC1)\n",
    "\n",
    "print(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T11:29:10.415649Z",
     "start_time": "2021-09-20T11:29:10.408995Z"
    },
    "code_folding": [
     0,
     1
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 0.66666667]\n",
      " [0.66666667 3.33333333]]\n",
      "[3.49071198 0.50928802]\n",
      "------------------------------\n",
      "Direction with maximum spread (PC1):\n",
      "[-0.22975292 -0.97324899]\n"
     ]
    }
   ],
   "source": [
    "# Practice\n",
    "data = np.array([[0,2], \n",
    "                 [0,-2],\n",
    "                 [1,1],\n",
    "                 [-1,-1]\n",
    "                ])\n",
    "cov  = np.cov(data.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "max_idx = np.argmax(eigenvalues)\n",
    "\n",
    "print(cov)\n",
    "print(eigenvalues[::-1])\n",
    "print('-'*30)\n",
    "print('Direction with maximum spread (PC1):')\n",
    "print(eigenvectors[:, max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T08:09:42.922047Z",
     "start_time": "2021-09-21T08:09:42.916259Z"
    },
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "data = np.array([\n",
    "                 [1,1], \n",
    "                 [1,-1], \n",
    "                 [-1,1]\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T08:09:45.247426Z",
     "start_time": "2021-09-21T08:09:45.243057Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  0]\n",
      " [ 0  2 -2]\n",
      " [ 0 -2  2]]\n"
     ]
    }
   ],
   "source": [
    "# Find the Gram matrix, B = XX^T\n",
    "X = np.copy(data)\n",
    "B = X.dot(X.T)\n",
    "\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T08:11:29.757392Z",
     "start_time": "2021-09-21T08:11:29.752394Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 0. 2.]\n",
      "[[ 0.          0.          1.        ]\n",
      " [-0.70710678  0.70710678  0.        ]\n",
      " [ 0.70710678  0.70710678  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalue decomposition of B\n",
    "eigenvalues, eigenvectors = np.linalg.eig(B)\n",
    "\n",
    "print(eigenvalues)\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T08:19:29.830127Z",
     "start_time": "2021-09-21T08:19:29.824212Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -1.41421356  1.41421356]\n"
     ]
    }
   ],
   "source": [
    "# Find 1 dimension for projection\n",
    "idx        = np.argmax(eigenvalues)\n",
    "scale      = np.amax(eigenvalues)\n",
    "projection = eigenvectors[:, idx]\n",
    "Y          = projection*scale**0.5\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-21T08:17:46.718214Z",
     "start_time": "2021-09-21T08:17:46.712963Z"
    }
   },
   "source": [
    "## SNE / tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
