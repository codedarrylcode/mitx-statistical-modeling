{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Methods of Classification on High-Dimensional Data\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Discriminant Analysis** (Linear / Quadratic)<br>\n",
    "\n",
    "1. Quadratic Discriminant Analysis\n",
    "    \n",
    "    Maximize $P(C = c | X = x)$ by Mahalanobis Distance, assuming a Gaussian for each class.\n",
    "    \n",
    "    Find a quadratic decision boundary where $P(C = 0 | X = x) = P(C = 1 | X = x)$\n",
    "\n",
    "    \n",
    "2. Linear Discriminant Analysis\n",
    "\n",
    "    Assumes same covariance matrix for all classes and therefore decision boundaries are linear.\n",
    "    \n",
    "    If covariance matrix is assumed to be, $\\Sigma = \\sigma^2 \\cdot I$, then it assumes a circular covariance and therefore a particular observation's probability of belonging to a class depends on its Euclidean distance to the class mean, i.e. _K Means_\n",
    "    \n",
    "\n",
    "3. Reduced-rank LDA, aka Fisher's LDA\n",
    "\n",
    "    Idea: $\\mu_1, \\dots, \\mu_K \\in R^p$ lie in a linear subspace of dim $K - 1$ (usually $p >> k$). Combine LDA with PCA, i.e. perform PCA on class means.\n",
    "    \n",
    "    The maximum number of linear discriminants: $min(p, K-1)$\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "$log(\\frac{p}{1-p}) = \\beta_0 + \\beta^T X$ (Solve for $\\beta_0$, $\\beta$ with MLE)\n",
    "\n",
    "$\\therefore p = \\frac{exp(\\beta_0 + \\beta^T X)}{1 + exp(\\beta_0 + \\beta^T X)}$\n",
    "\n",
    "Choose $C = 1$ if $p > 0.5$\n",
    "\n",
    "Assumes shape of probability distribution of categories is known (Gaussian).\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Support Vector Machines (SVM)**<br>\n",
    "\n",
    "Find the hyperplane that maximizes the margin between binary classes. \n",
    "\n",
    "Given training data: $(x_1, y_1), \\dots, (x_n, y_n)$ with $x_i \\in R^p$, $y_i \\in$ {-1, 1}, determine a hyperplane $wx - b = 0$ that maximizes the distance to the nearest point $x_i$ from each group.\n",
    "\n",
    "If **perfect classification** is possible:\n",
    "\n",
    "Maximize the margin $\\frac{2}{\\lVert w \\rVert_2}$, or equivalently, minimize $\\lVert w \\rVert_2$ such that $y_i (w x_i - b) \\geq 1$ for all $i$\n",
    "\n",
    "If **imperfect classification** (usually the case):\n",
    "\n",
    "Maximize margin AND minimize sum of classification errors, minimize $\\lVert w \\rVert_2 + \\lambda \\sum_{i=1}^{n} \\epsilon$ where $\\epsilon$ is the classification error, such that $y_i (w x_i - b) \\geq 1 - \\epsilon_i$ for all $i$\n",
    "\n",
    "SVM is a non-parametric method, i.e. no probability distribution, and therefore does not assume the shape of the distribution. Needs labeled binary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
