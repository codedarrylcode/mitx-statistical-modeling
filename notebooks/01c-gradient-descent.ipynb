{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general method for finding the minima of any function is set the first derivative to zero and solve for its parameters\n",
    "\n",
    "$\\nabla f(w) = 0$\n",
    "\n",
    "For example, in Ordinary Least Squares, we would like to estimate the weights, $\\hat{w}$, that minimizes the loss function:\n",
    "\n",
    "$\\hat{w} = \\displaystyle\\arg \\min_{\\substack{w}} \\sum_{i=1}^{N}(y_i - x_i w)^2$:\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Convexity**<br>\n",
    "\n",
    "A function, $f$, is convex if at each point, if $f$ is twice differentiable: $f$ is convex for all $w$\n",
    "\n",
    "$\\nabla^2 f(w)$ is a positive semidefinite (PSD) Hessian matrix (in 1D: $f''(w) \\geq 0$)\n",
    "\n",
    "Any matrix, $A$, is PSD if:\n",
    "1. $v^T A v \\geq 0$ for any vector, $v$, or\n",
    "1. $A$ has eigenvalues $\\geq 0$\n",
    "\n",
    "If $f''(w) = 0$ then $w$ might be a **saddle point** and not a minma or maxima. In multiple dimensions, a Hessian matrix with a mixture of positive and negative eigenvalues (an indefinite matrix) suggest that the loss function curves upwards and downwards and is the definition of a saddle point.\n",
    "\n",
    "$\\therefore$ if the second derivative is non-negative everywhere, $f''(w) \\geq 0$ for all $w$, then it has a unique global minimum and is a convex function\n",
    "\n",
    "<hr>\n",
    "\n",
    "**General workings of Gradient Descent**<br>\n",
    "Solving $\\nabla f(w) = 0$ directly might be difficult. <br>\n",
    "\n",
    "Gradient descent uses an iterative way to determine $w$ that would minimize the function by starting with an arbitrary $w$ and then goes closer to the crtiical point with each iteration.\n",
    "\n",
    "General scheme:\n",
    "1. Start with some $w^0$, for $t = 0, 1, \\ldots$\n",
    "1. $w^{t+1} \\leftarrow w^t + a_t d^t$ where $a_t$ is the step size and $d^t$ is the direction of the descent\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Direction, $d^t$**<br>\n",
    "To move one step in the right direction, we have to estimate a quadratic function, $g_t(u)$, that is larger than $f_t(w)$ for all values of $w$.\n",
    "\n",
    "$g_t (u) = f_t (w^t) + f_t' (w^t) (u - w^t) + \\frac{L}{2} (u - w^t)^2$ \n",
    "\n",
    "where $L$ defines the step size and should be the largest eigenvalue of the Hessian\n",
    "\n",
    "such that:\n",
    "1. $g_t (w) \\geq f_t (w)$ for all values of $w$\n",
    "2. $g_t (w^t) = f_t (w^t)$\n",
    "\n",
    "<img alt=\"Quadratic Minimization\" src=\"assets/quadratic_minimization.png\" width=\"300\">\n",
    "\n",
    "Given $g_t (u)$, find derivative with respect to $u$ and set it to zero to find $u$ that minimizes $g_t (u)$\n",
    "\n",
    "$u_t = w^t - \\frac{1}{L}f'(w^t) = w^{t+1}$\n",
    "\n",
    "Example: *Least Squares Regression*\n",
    "\n",
    "$w^{t+1} \\leftarrow w^t - \\alpha_t \\nabla f(w^t)$, where $\\alpha_t$ is the step size\n",
    "\n",
    "Gradient for squared loss: \n",
    "\n",
    "$\\nabla_w (\\sum_{i=1}^{N} (y_i - x_i w)^2) = \\sum_{i=1}^{N} \\nabla_w (y_i - x_i w)^2$\n",
    "\n",
    "$= -2 \\sum_{i=1}^{N} (y_i - x_i w) \\cdot x_i^T$\n",
    "\n",
    "$w^{t+1} = w^{t} + 2 \\alpha_t \\sum_{i=1}^{N} (y_i - x_i w) \\cdot x_i^T$\n",
    "\n",
    "$\\therefore w^{t+1}$ will be a combination of $x_i$'s\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Step sizes, $\\alpha_t$**\n",
    "\n",
    "With step size, $\\alpha_t = \\frac{1}{L}$, $L$ may be hard to compute as it is the maximum eigenvalue of the Hessian. \n",
    "\n",
    "Suppose each step will further minimize the loss, then the formula below holds:\n",
    "\n",
    "$f(w^{t+1}) \\leq f(w^t) - \\frac{1}{2L} \\lVert f(w^t) \\rVert^2$\n",
    "\n",
    "Start with an optimistic $\\alpha_t$, check if the above equation holds. If yes, use $\\alpha_t$ else try $\\frac{\\alpha}{2}$ and check again.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Stochastic Gradient Descent**<br>\n",
    "Most frequently used variant of gradient descent for most ML applications.\n",
    "\n",
    "Problem: In linear regression, each iteration is a computation of a gradient of the sum of residuals across N data points. When N is large, then this becomes computationally expensive.\n",
    "\n",
    "Solution: Instead of taking the sum of all data points, estimate the sum with a few (or one) data point(s)\n",
    "\n",
    "1. Start with some $w_0$, for $t = 0, 1, 2, \\ldots$\n",
    "1. Draw data point uniformly at random\n",
    "1. Compute the next step: $w^{t+1} \\leftarrow w^t - \\alpha_t \\nabla f_i(w^t)$\n",
    "1. Step size, $\\alpha_t$ should shrink ($\\approx \\frac{1}{t+1}$) as $t$ increases\n",
    "\n",
    "In practice, stochastic gradient descent may not minimize the loss function in a given step but on expectation should minimize and converge to the minimum with the same number of iterations with each iteration being much less computationally expensive.\n",
    "\n",
    "<img alt=\"Gradient Descent vs Stochastic Gradient Descent\" src=\"assets/gd_vs_sgd.png\" width=\"300\">\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
