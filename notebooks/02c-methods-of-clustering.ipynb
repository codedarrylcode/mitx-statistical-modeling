{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with High-Dimensional Data\n",
    "Given $x_i \\in R^p$, group data points together so that similar observations are in the same group and observations between different groups are dissimilar\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means**<br>\n",
    "\n",
    "Starts with a pre-selected number of clusters, $K$, and minimizes the **within group sums of squares** (WGSS)\n",
    "\n",
    "$WGSS = \\sum_{k = 1}^{K} \\sum_{x^{(i}, x^{(j)} \\in C_k} d(x^{(i)}, x^{(j)})^2 = \\sum_{k = 1}^{K} 2 N_k \\sum_{x^{(i} \\in C_k} \\lVert x_i - u_k \\rVert_2^2$\n",
    "\n",
    "where $k$ indexes the $K$ different clusters, $C_k$ denotes the $k$-th cluster, $d(x^{(i)}, x^{(j)})$ is the Euclidean distance between data points and $N_k$ is the number of observations in cluster $K$\n",
    "\n",
    "This is the same as maximizing the **between group sum of squares** (BGSS)\n",
    "\n",
    "*Algorithm*:\n",
    "1. Initialize the $K$ means {$\\mu_k$}$_{k = 1, \\dots, K}$ to random positions\n",
    "2. Repeat the two steps below until algorithm converges\n",
    "    1. Cluster assignment: Assign each point to the closest centroid $\\mu_k$\n",
    "    2. Centroids update: Update all centroids $\\mu_k$ based on all the data points assigned to $C_k$\n",
    "    \n",
    "$K$-means does not guarantee convergence to the global minimum, depending on random initialization of cluster centroids. To converge on global minimum, run several iterations with different initialization centroids.\n",
    "\n",
    "*Limitations*:\n",
    "- Sensitive to local outliers\n",
    "- Cluster centroids are not necessarily data points\n",
    "- Assumes $\\Sigma_k = \\sigma^2 \\cdot I$, i.e. zero covariances between dimensions\n",
    "\n",
    "Fix by using *medoids* instead of *means* for robustness to outliers and ensuring cluster centroids are data points\n",
    "\n",
    "*Methods for choosing $K$*\n",
    "1. Elbow plot\n",
    "    - Plot WGSS vs $K$\n",
    "    - Choose $K$ where second derivative is closest to zero\n",
    "    \n",
    "    \n",
    "2. Downstream accuracy\n",
    "    - Choose $K$ such that downstream measurements is minimized, e.g. RMSE\n",
    "  \n",
    "  \n",
    "3. Business contraints\n",
    "    - Let business requirements decide the number of clusters expected, e.g. marketing campaign resource is only available for K number of selected campaigns\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Gaussian Mixture Models (GMM)**<br>\n",
    "Considers the covariance of the dimensions and computes a soft probabilistic assignment of each observation to a cluster rather than a hard assignment.\n",
    "\n",
    "$P(x) = \\sum_{k=1}^{K} P(C_k) P(x | C_k)$\n",
    "\n",
    "Parameter estimates of $P(C_k), \\mu_k, \\Sigma_k$ are usually found using the **Expectation-Maximization** (EM) algorithm.\n",
    "\n",
    "*Algorithm*:\n",
    "1. **E-step**: Compute $P(C_k | x^{(i)}) = \\frac{P(C_k) \\cdot P(x^{(i)} | C_k)}{P(x^{(i)})}$\n",
    "\n",
    "2. **M-step**: Update the parameter estimates iteratively\n",
    "\n",
    "    $P(C_k) = \\frac{1}{n} \\sum_{i=1}^{n} P(C_k|x^{(i)})$\n",
    "    \n",
    "    $\\mu_k  = \\sum_{i=1}^{n} x^{(i)} \\frac{P(C_k | x^{(i)})}{\\sum_{i=1}^{n} P(C_k | x^{(i)})}$\n",
    "\n",
    "\n",
    "3. Repeat steps 1 and 2 until there is no noticeable change in the actual likelihood computed\n",
    "\n",
    "\n",
    "Number of clusters is found by maximizing the *Bayesian Information Criterion* (BIC)\n",
    "\n",
    "$BIC =$ log-likelihood - $\\frac{log(n)}{2}$ (# of parameters)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Hierarchical Clustering**<br>\n",
    "\n",
    "1. **Agglomerative clustering**\n",
    "    \n",
    "    Build up clusters from individual observations until there is only one cluster at the top of the tree.\n",
    "    \n",
    "    Starts with 1 data point per cluster, and at each consequent stage, merges pairs of clusters that are the closest together according to a dissimilarity measure between clusters.\n",
    "    \n",
    "    The merging is depicted by a tree, known as a [dendrogram](https://en.wikipedia.org/wiki/Dendrogram). The bottom-most level has $n$ clusters (of 1 observation each) and as merging occurs, the number of clusters decreases and the top-most level has only 1 cluster.\n",
    "    \n",
    "    <img alt=\"Hierarchical Clustering\" src=\"assets/dissimilarity_vs_clusters.png\" width=\"300\">\n",
    "    \n",
    "    To choose which pair of clusters to merge at each stage, we need to define a dissimilarity measure between clusters. (See below for examples)\n",
    "    \n",
    "    \n",
    "2. **Divisive clustering**\n",
    "\n",
    "    Start with whole group of observations and split off clusters\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Examples of distance metrics between clusters:\n",
    "- Single linkage (minimum distance)\n",
    "\n",
    "    $d(C_r, C_s) = \\displaystyle \\min_{\\substack{x^{(i)} \\in C_r, x^{(j)} \\in C_s}} d(x^{(i)}, x^{(j)})$\n",
    "    \n",
    "    \n",
    "- Complete linkage (maximum distance) (*Default*)\n",
    "\n",
    "    $d(C_r, C_s) = \\displaystyle \\max_{\\substack{x^{(i)} \\in C_r, x^{(j)} \\in C_s}} d(x^{(i)}, x^{(j)})$\n",
    "    \n",
    "\n",
    "- Average linkage (average distance)\n",
    "\n",
    "    $d(C_r, C_s) = \\frac{1}{n_r} \\frac{1}{n_s} \\sum_{x^{(i)} \\in C_r} \\sum_{x^{(j)} \\in C_s} d(x^{(i)}, x^{(j)})$\n",
    "    \n",
    "    \n",
    "<img alt=\"Clustering by various distance metrics\" src=\"assets/distance_metrics.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Choosing the number of clusters:\n",
    "- Find the largest vertical drop in the tree\n",
    "\n",
    "<img alt=\"Dendrogram\" src=\"assets/dendrogram.png\" width=\"300\">\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Density-based spatial clustering of applications with noise (DBSCAN)**\n",
    "\n",
    "Cluster points that are close to each other in a dense region and leave out points that are in low density regions.\n",
    "\n",
    "To perform DBSCAN, two parameters are required:\n",
    "1. $\\epsilon$, distance between connected points\n",
    "2. $k$, core strength\n",
    "\n",
    "Two points be connected if they are within a distance, $\\epsilon$ of one another. Two points are placed into the same cluster *iff* there is a connecting path between them consisting of only core points (a point that is coonnected to at least $k$ other points), except possibly at the ends of the path.\n",
    "\n",
    "In the figure below, the blue points are core points for core strength $k = 4$. In each cluster, each non-core (black) point is connected to a core point (blue). The non-connected points are *outliers*\n",
    "\n",
    "<img alt=\"DBSCAN Core Points\" src=\"assets/dbscan_core_points.png\" width=\"300\">\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Evaluating quality of clustering: Silhouette Plots**\n",
    "\n",
    "Compute for each sample $x^{(i)}$:\n",
    "\n",
    "- $a(x^{(i)}) =$ Average dissimilarity between $x^{(i)}$ and all other points in its cluster\n",
    "\n",
    "- $b(x^{(i)}) =$ Average dissimilarity between $x^{(i)}$ and the closest cluster it does not belong to\n",
    "\n",
    "- $S(x^{(i)}) \\in [-1, 1]$ with\n",
    "    \n",
    "    $S(x^{(i)}) = \\frac{(b(x^{(i)}) - a(x^{(i)}))}{\\max(a(x^{(i)}), b(x^{(i)}))}$\n",
    "    \n",
    "\n",
    "Find average silhouette width across all points where: \n",
    "\n",
    "$S(x^{(i)})$ large: well-clustered; $S(x^{(i)})$ small: badly clustered; $S(x^{(i)}) < 0$: wrongly clustered \n",
    "\n",
    "<img alt=\"Silhouette Plot\" src=\"assets/silhouette_plot.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
