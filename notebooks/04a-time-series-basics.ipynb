{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Series Analysis: The Basics\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Understanding**<br>\n",
    "A collection of observations (realization of random variables) $x_1, \\dots, x_n \\in \\mathbb {R}$ indexed by time (fixed time intervals)\n",
    "\n",
    "- Components: $X_t = T_t + S_t + Z_t$, where $T_t$, $S_t$, $Z_t$ are trend, seasonality and noise respectively while subscript $t$ denotes a deterministic sequence of time stamps that are regularly spaced with equal intervals between any two adjacent stamps\n",
    "\n",
    "- Smoothness: Correlation in time (previous values predicts current value in time). Less smoothness refers to lower correlation in time\n",
    "\n",
    "The most important feature of time series data is that we do not assume $iid$ random variables. Most time series data are dependent, as past realizations of the RV influence future observations.\n",
    "\n",
    "A general probabilistic model to describe a real world phenomena that evolves continously in time is called a stochastic process, which is simply a collection of random variables $X_t$ indexed by either a continuous or discrete time realization of a stochastic process.\n",
    "\n",
    "A time series data set can be thought of as a single realization of a stochastic process. Each random variable $X_t$ has a marginal distribution, $P_t$ and the stochastic process of the collection of all {$X_t$} can be thought of as a joint distribution of all $X_t$'s\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Deterministic dependencies: trend, seasonality**\n",
    "\n",
    "Decomposition of the deterministic dependence of a time series, let $\\mu_X (t)$ denote the mean function of the time series.\n",
    "\n",
    "$\\mu_X (t) = m_X (t) + s_X (t) + W_t$\n",
    "\n",
    "where $m_X (t)$, $s_X (t)$, $W_t$ denotes the trend (*non-constant, non-cyclical*), seasonality (*cyclical, periodic*) and white noise time series (iid, $N(0, \\sigma^2)$) components respectively\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Stationarity / Stochastic Dependence**\n",
    "\n",
    "- Marginal mean per time stamp: $\\mu_X (t) = \\mathbb {E}[X_t]$\n",
    "\n",
    "- Margin variance: $var(X_t) = \\mathbb {E} [(X_t - \\mu_X (t))^2]$\n",
    "\n",
    "- Autocovariance: $\\gamma_X (s, t)$, where $s$, $t$ are different time points\n",
    "\n",
    "    $\\gamma_X (s, t) = cov(X_s, X_t) = \\mathbb {E} [(X_s - \\mu_X (s))(X_t - \\mu_X (t)]$\n",
    "    \n",
    "    To estimate the covariance between a given time gap (*say, 50*) then take all stationarized observations with such gaps. For example, the covariance between $T_1$ to $T_5$ should be the same as $T_{51}$ to $T_{55}$.\n",
    "    \n",
    "    Typically, the autocovariance decays as the time distance between observation increases while the variance of $X_t$ stays constant over time.\n",
    "    \n",
    "\n",
    "The idea of stationarity is to remove the trend and seasonality such that observations of the process are representative of all possible realizations of the random variable.\n",
    "\n",
    "$\\therefore$ We need conditions that would allow us to estimate population parameters of the whole process (e.g. expectations, variances, correlations) with time averages over a single realization of the process. Weak stationarity implies that the first and second moments (mean, variance) have to be the same for all realizations of the random variables. Strong stationarity implies all moments.\n",
    "\n",
    "A time series {$X_t$} is strongly stationary if the joint distribution of $X_t, \\dots, X_{t+n}$ is the same as the joint distribution of $X_{t+h}, \\dots, X_{t+n+h}$ for all integers $n$, time stamps $t$ and time shfiters $h$.\n",
    "\n",
    "Weak stationarity only requires that the first two moments (mean, variance/covariance) be constant in time, \n",
    "\n",
    "$\\mathbb {E} [X_t] = \\mu_X$\n",
    "\n",
    "$Var(X_t) = \\sigma_X^2$\n",
    "\n",
    "$Cov(X_s, X_t) = \\gamma_X (\\lvert s - t \\rvert)$ for all time stamps, $s$, $t$\n",
    "\n",
    "Detect non-stationarity using the following:\n",
    "\n",
    "- Visualizations\n",
    "- Autocovariance: trends/seasonal effects, changes in distribution\n",
    "    - If stochastic dependencies (i.e. correlations) in the time series decays sufficiently fast as time distance gets larger then it satisfies mild conditions for asymtoptic behaviour as in CLT and LLN for iid data and we have good estimators for the mean, variance and autocovariance \n",
    "\n",
    "\n",
    "How to stationarize time series:\n",
    "\n",
    "- Removing trend: \n",
    "    - Regression, estimate $X_t' = X_t - \\hat{T_t}$ where $\\hat{T_t} = \\hat{\\beta_1}t + \\beta_0$\n",
    "    \n",
    "    \n",
    "- Remove seasonality:\n",
    "    - Regression, estimate $X_t' = X_t - \\hat{S_t}$\n",
    "    - Subtract weekly/monthly/yearly averages, estimate a vector of $\\hat\\mu_{weekday}$ and subtract this from the time series\n",
    "    - Fourier analysis\n",
    "    - Smoothing, e.g. by exponential moving averages (give more weight to recent data), $Y_t = \\sum_{h = -k}^{k} \\alpha_h X_{t+h}$\n",
    "    \n",
    "\n",
    "- Non-linear transformations (for e.g. log transformations for increasing variance over time), $Y_t = \\log (X_t - \\mu_t)$, $Y_t = \\sqrt{X_t - \\mu_t}$\n",
    "\n",
    "\n",
    "- Differencing between previous values, $Y_t = \\nabla X_t = X_t - X_{t-1}$ (removes linear trend)\n",
    "    - Removing quadratic trend by differencing twice, $\\nabla^2 X_t = \\nabla X_t - \\nabla X_{t-1}$\n",
    "    - {$X_t$} is integerated of order p: $\\nabla^p X_t$ is stationary, where $p$ is limited by the length of the time series. The higher $p$ is then the lesser number of data points are remaining in the staionarized data\n",
    "    - Differencing the data is also a standard method to transform a time series with persistent stochastic dependencies (seasonality) into a stationary time series\n",
    "    \n",
    "    \n",
    "****\n",
    "\n",
    "**Properties of autocovariance, $\\gamma_X (s, t)$**\n",
    "\n",
    "Autocovariance: $\\gamma_X (s,t) = cov(X_s, X_t) = \\mathbb {E} [(X_s - \\mu (s)) (X_t - \\mu (t))]$\n",
    "\n",
    "Autocorrelation (ACF): $\\rho_X (s,t) = \\frac{\\gamma_X (s, t)}{\\sqrt{\\gamma_X (s,s) \\cdot \\gamma_X (t,t)}}$\n",
    "\n",
    "- Symmetric, $\\rho_X (s, t) = \\rho_X (t, s)$\n",
    "- Measures linear dependence of $X_t$, $X_s$\n",
    "- Relates to smoothness (if decay of autocovariance is slow then time series is smooth)\n",
    "- For weakly stationary series: $\\gamma_X (t, t+h) = \\gamma_X (0, h) =: \\gamma_X(h)$\n",
    "\n",
    "<img alt=\"ACF Decay (Smooth)\" src=\"assets/acf_smoothness.png\" width=\"300\">\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "**Sample estimates for stationary series**\n",
    "\n",
    "- Mean: $\\hat\\mu = \\bar{x} = \\frac{1}{n} \\sum_{t=1}^{n} x_t$\n",
    "\n",
    "\n",
    "- Variance: $\\frac{1}{n} \\sum_{t=1}^{n} (x_t - \\bar{x})^2$\n",
    "\n",
    "\n",
    "- Autocovariance: $\\hat\\gamma_X (h) = \\frac{1}{n} \\sum_{t=1}^{n-\\lvert h \\rvert} (x_t - \\bar{x}) (x_{t+\\lvert h \\rvert} - \\bar{x})$ for $-n < h < n$ where $h$ is the time gap\n",
    "\n",
    "\n",
    "- Autocorrelation: $\\hat\\rho_X (h) = \\hat\\gamma_X (h) / \\hat\\gamma_X (0)$ for $-n < h < n$ where $\\hat\\gamma_X (0)$ is the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:54:07.441564Z",
     "start_time": "2021-11-02T09:54:07.435031Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.666666666666667\n",
      "1 4.444444444444445\n",
      "2 2.3333333333333335\n",
      "3 0.4444444444444444\n",
      "4 -1.1111111111111112\n",
      "5 -2.2222222222222223\n",
      "6 -2.7777777777777777\n",
      "7 -2.6666666666666665\n",
      "8 -1.7777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Compute autocovariances\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acovf\n",
    "\n",
    "data = np.array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
    "auto_covariances = acovf(data)\n",
    "\n",
    "for h, gamma in enumerate(auto_covariances, start = 0):\n",
    "    print(h, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:55:16.344583Z",
     "start_time": "2021-11-02T09:55:16.338596Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5\n",
      "1 0.0\n",
      "2 -0.375\n",
      "3 0.0\n",
      "4 0.25\n",
      "5 0.0\n",
      "6 -0.125\n",
      "7 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute autocovariances\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acovf\n",
    "\n",
    "data = np.array([-1, 0, 1, 0, -1, 0, 1, 0])\n",
    "auto_covariances = acovf(data)\n",
    "\n",
    "for h, gamma in enumerate(auto_covariances, start = 0):\n",
    "    print(h, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
